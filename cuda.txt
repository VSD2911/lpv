

    !pip install git+https://github.com/afnan47/cuda.git

    Collecting git+https://github.com/afnan47/cuda.git
      Cloning https://github.com/afnan47/cuda.git to /tmp/pip-req-build-6enz9loj
      Running command git clone --filter=blob:none --quiet https://github.com/afnan47/cuda.git /tmp/pip-req-build-6enz9loj
      Resolved https://github.com/afnan47/cuda.git to commit aac710a35f52bb78ab34d2e52517237941399eff
      Preparing metadata (setup.py) ... e=NVCCPlugin-0.0.2-py3-none-any.whl size=4289 sha256=8bf7d4afb38dfec3c9ad988a1f1ad9b0a0316bc495cdb451dbdaa613bb290a94
      Stored in directory: /tmp/pip-ephem-wheel-cache-vgp30bxb/wheels/aa/f3/44/e10c1d226ec561d971fcd4b0463f6bff08602afa928a3e7bc7
    Successfully built NVCCPlugin
    Installing collected packages: NVCCPlugin
    Successfully installed NVCCPlugin-0.0.2

    %load_ext nvcc_plugin

    created output directory at /content/src
    Out bin /content/result.out

    %%cu
    #include <iostream>
    using namespace std;


    // CUDA code to multiply matrices

    __global__ void multiply(int* A, int* B, int* C, int size) {
        // Uses thread idices and block indices to compute each element
        int row = blockIdx.y * blockDim.y + threadIdx.y;
        int col = blockIdx.x * blockDim.x + threadIdx.x;

        if (row < size && col < size) {
            int sum = 0;
            for (int i = 0; i < size; i++) {
                sum += A[row * size + i] * B[i * size + col];
            }
            C[row * size + col] = sum;
        }
    }

    # gives random values to matrix;
    void initialize(int* matrix, int size) {
        for (int i = 0; i < size * size; i++) {
            matrix[i] = rand() % 10;
        }
    }

    #prints the matrix;
    void print(int* matrix, int size) {
        for (int row = 0; row < size; row++) {
            for (int col = 0; col < size; col++) {
                cout << matrix[row * size + col] << " ";
            }
            cout << '\n';
        }
        cout << '\n';
    }


    int main() {
        int* A, * B, * C;

        int N = 2;
        int blockSize =  16;

        int matrixSize = N * N;
        size_t matrixBytes = matrixSize * sizeof(int);

        A = new int[matrixSize];
        B = new int[matrixSize];
        C = new int[matrixSize];

        initialize(A, N);
        initialize(B, N);
        cout << "Matrix A: \n";
        print(A, N);

        cout << "Matrix B: \n";
        print(B, N);


        int* X, * Y, * Z;
        // Allocate space
        cudaMalloc(&X, matrixBytes);
        cudaMalloc(&Y, matrixBytes);
        cudaMalloc(&Z, matrixBytes);

        // Copy values from A to X
        cudaMemcpy(X, A, matrixBytes, cudaMemcpyHostToDevice);

        // Copy values from A to X and B to Y
        cudaMemcpy(Y, B, matrixBytes, cudaMemcpyHostToDevice);

        // Threads per CTA dimension
        int THREADS = 2;

        // Blocks per grid dimension (assumes THREADS divides N evenly)
        int BLOCKS = N / THREADS;

        // Use dim3 structs for block  and grid dimensions
        dim3 threads(THREADS, THREADS);
        dim3 blocks(BLOCKS, BLOCKS);

        // Launch kernel
        multiply<<<blocks, threads>>>(X, Y, Z, N);

        cudaMemcpy(C, Z, matrixBytes, cudaMemcpyDeviceToHost);
        cout << "Multiplication of matrix A and B: \n";
        print(C, N);

        delete[] A;
        delete[] B;
        delete[] C;

        cudaFree(X);
        cudaFree(Y);
        cudaFree(Z);

        return 0;
    }

    Matrix A: 
    3 6 
    7 5 

    Matrix B: 
    3 5 
    6 2 

    Multiplication of matrix A and B: 
    45 27 
    51 45 

    %%cu

    #include <iostream>
    #include <cstdlib>

    // CUDA kernel for vector addition
    __global__ void vectorAdd(int *a, int *b, int *result, int n) {
        int tid = threadIdx.x + blockIdx.x * blockDim.x;
        if (tid < n) {
            result[tid] = a[tid] + b[tid];
        }
    }

    int main() {
        int *a, *b, *c;
        int *a_dev, *b_dev, *c_dev;
        int n = 20; // Example large number

        a = new int[n];
        b = new int[n];
        c = new int[n];
        int size = n * sizeof(int);

        cudaMalloc(&a_dev, size);
        cudaMalloc(&b_dev, size);
        cudaMalloc(&c_dev, size);

        // Initialize arrays with values
        for (int i = 0; i < n; i++) {
            a[i] = 1;
            b[i] = 2;
        }

        // Copy data from host to device
        cudaMemcpy(a_dev, a, size, cudaMemcpyHostToDevice);
        cudaMemcpy(b_dev, b, size, cudaMemcpyHostToDevice);

        int threads = 1024;
        int blocks = (n + threads - 1) / threads;

        // Execute kernel on GPU
        vectorAdd<<<blocks, threads>>>(a_dev, b_dev, c_dev, n);

        // Copy result back to host
        cudaMemcpy(c, c_dev, size, cudaMemcpyDeviceToHost);

        // Verify the result
        bool success = true;
        for (int i = 0; i < n; i++) {
            std::cout<<c[i]<< " ";
            if (c[i] != a[i] + b[i]) {
                success = false;
                std::cout << "Error at position " << i << std::endl;
                break;
            }
        }

        if (success) {

            std::cout << "  Vector addition successful!" << std::endl;
        }

        // Free memory
        cudaFree(a_dev);
        cudaFree(b_dev);
        cudaFree(c_dev);
        delete[] a;
        delete[] b;
        delete[] c;

        return 0;
    }

    3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3   Vector addition successful!


//cstdlib is for general-purpose functions, such as memory allocation and random number generation
//This is a CUDA kernel function. Kernels are functions that are executed on the GPU. This particular kernel is for adding two vectors (a and b) element-wise and storing the result in result.

threadIdx.x represents the index of the thread within its block.
blockIdx.x represents the index of the block within the grid.
blockDim.x represents the number of threads per block.
tid calculates a unique global index for each thread. If tid is less than n (the size of the vectors), it performs the addition and stores the result.
